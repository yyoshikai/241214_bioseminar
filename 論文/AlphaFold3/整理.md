

# Input embedder


# Template module

# MSA module
SM3.3

トランクでは, MSAのembedding blockが大幅に小さく簡単になるなど(SM3.3), MSAの処理が大幅に強調されなくなっている。
AF2の元々のevoformerと比較すると, (何の?)ブロックの数が4つに減少し, MSA表現の処理は計算量の少ないpair-weighted averagingになっており, その後のステップではペア表現のみが用いられる。


# PairFormer
図2a
SM3.6
主要な処理ブロックとしては, AF2のevoformerが"pairformer"(図2a, SM3.6)になっている。
pairformerはペア表現と単一の表現のみを処理する。MSA表現はここでは保持されず, 全てのMSA情報はペア表現を通じて伝達される。
ペアの処理とブロック数(48)はAF2から大まかには変わっていない。
得られたペア表現と単一配列の表現は, 入力の表現と共に, AF2のstructure moduleにあたる新しいdiffusion modelに渡される。

# Diffusion module


拡散モジュール(図2b, SM3.7)は, 原子座標そのもの, および疎な抽象的なトークン表現(confidenceなど座標以外のheadに対応するもの?)を直接操作し, 回転するフレーム等は用いない。
私たちは, AF2の構造モジュールの大部分を簡略化しても, 予測精度には控えめな影響しかないことを発見しており, (タンパク質について?)骨格フレームと残基のねじれ表現を保持すると一般の原子グラフに対してはかなり処理が複雑になる。
また, AF2では出力構造の化学的妥当性を保つため, 立体化学的な法則の違反に対する損失項を慎重に調整する必要があった。
私たちは, ノイズを加えた原子座標から本当の座標を予測するようモデルを訓練する標準的な拡散モデルの手法[33]を使った。
このような課題設定では, 様々な長さのタンパク質構造を学習する必要がある。それによって, 小さなノイズを除去するタスクがとても局所的な立体化学の理解を重視し, 大きなノイズを除去するタスクが大きなスケールでの構造を重視する。
推論時には, ランダムに生成したノイズを繰り返しノイズ除去することで最終的な構造を出力する。
重要なことは, これは答えの分布を出力する生成的な学習過程であるということである。
これはつまり, 局所的な構造について, モデルに自信がなくても細かな構造を出力する(例えば, 側鎖の結合についての構造(?))ということである。
このため, 一般的なリガンドを全て処理しつつ, ねじれ角ベースで残基を表現したり, 構造の法則の違反項の設定を回避できる。
最近の研究[34]と同様に, 私たちは分子全体の回転・並進に対する不変性・同変性は必要ないことを発見しており, モデルの簡略化のためそれらを省いた。

# confidence head
また, 私たちは最終的な構造の原子レベル, 及びペアごとの誤差を予測するconfidenceの指標も開発した。
AF2では, 学習中に構造モジュールの誤差を直接回帰する(=回帰タスクとして学習する?)ことでこれを行っていた。
しかし, 拡散の学習においては全体の構造生成ではなく拡散の各段階が学習されるため, この方法は使えない(2c)。
これを解決するため, 学習中に全ての構造を生成するための(通常より大きなstepを使った)拡散'rollout'過程を開発した(2c)。
予測されたこの構造を使い, 正解の鎖(の各残基?)とリガンド(の各原子?)を並べ替え, 確信度予測headを学習させるための予測精度の評価指標を計算した。
確信度headは, ペア表現を使ってF2と同様に modified mocal distance difference test(pLDDT)とpredicted aligned error(PAE)行列を予測するほか, 予測構造の距離行列と正解の構造の距離行列の誤差であるdistance error matrix(PDE)も予測する(詳細はSM4.3にある)。

# 学習方法

## AF-Multimerのcross distillation
生成的拡散モデルを採用する上で, いくつかの技術的課題に対処する必要があった。
最も大きな問題は, 生成モデルがhallucinationを起こしやすいということである。それによってモデルは構造化されていない領域でもそれらしい構造を生成する可能性がある。
この効果を打ち消すため, 私たちはAlphaFold-Multimer(v2.3)の予測結果で学習データを増幅するcross-distillationを行った。
AF-multimerの予測結果では, 構造化されていない領域は長いループで表現されることが多く, それを学習させることでAF3にそのようなふるまいを模倣させるよう教えた。
このcross-distillationにより, AF3のhallucinationは大幅に減少した(図E1にCAID 236ベンチマークデータセットについての無秩序な領域の予測結果を示している)。

## early stopping
図2dは, 最初の学習(=finetuningでない学習?)中, モデルは局所的な構造を早期に予測できるようになる(鎖内の精度の評価指標は全てすぐ上昇し, 性能の最大値の97%に20000step以内に到達する)ものの, 全体的な集まりを学習するにはそれよりかなり長いstepがかかる(界面についての評価指標はゆっくりと上昇し, タンパク質間のインターフェースのLDDTは60000step以上たってから最大性能の97%に到達している)。
AF3の開発中, 私たちはモデルのいくつかの能力が早い段階で最大に達し, その後他の能力が学習中であるにもかかわらず, (おそらくはこのモデルの能力に対して限られた数の学習データにオーバーフィッティングしたため)能力が低下し始めることを観測した。
これへの対処として, 私たちは対応する学習データのサンプリング確率を増やし/減らし(SM2.5.1), また最も良いモデルのチェックポイントを選ぶため, 上記全ての評価指標, 及びいくつかの追加の評価指標(表S7)の加重平均を使ってearly stoppingを行った。
   
## fine tuning
大きなcrop sizeを使ったfinetuningの段階では全ての指標が改善し, 特にタンパク質間の界面の精度が上昇した(図E2..どれ?)













